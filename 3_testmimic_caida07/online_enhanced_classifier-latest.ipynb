{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Flatten, TimeDistributed, Bidirectional\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "from datagenerator import DataGenerator\n",
    "\n",
    "# add\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable randomization\n",
    "import random\n",
    "seed_value=2020 #10 #\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# add\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "# # sess = tf.Session(config=config) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def loadConfig():\n",
    "#     with open('/home/wesleyjtann/ddos_research/test/light_config_b.yaml', \"r\") as ymlfile:\n",
    "    with open('./light_config.yaml', \"r\") as ymlfile: #GE, hulk, slowhttp, slowloris, heartbleed\n",
    "        cfg = yaml.load(ymlfile)\n",
    "    return cfg\n",
    "\n",
    "config = loadConfig()\n",
    "# artefact = 'artefact' #'artefact_b' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(seq, maxlen, tokenizer):\n",
    "    # Pads seq and slides windows\n",
    "    seq = seq[:maxlen]\n",
    "    seqX = np.append(tokenizer.word_index['<sos>'], seq)\n",
    "    seqY = np.append(seq, tokenizer.word_index['<eos>'])\n",
    "\n",
    "    x= pad_sequences([seqX],\n",
    "        maxlen=maxlen+1,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    y= pad_sequences([seqY],\n",
    "        maxlen=maxlen+1,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    return [x], [y]\n",
    "\n",
    "\n",
    "def getTokenizer(df_arr) :\n",
    "    ### Dictionary for Normal ###\n",
    "    tokenizer = Tokenizer(filters='', split='<sep>', oov_token='<OTHERS>' ,lower=True)\n",
    "    \n",
    "    for df in df_arr :\n",
    "        tokenizer.fit_on_texts(df['Input'].values)\n",
    "\n",
    "    tokenizer.fit_on_texts(['<SOS>'])\n",
    "    tokenizer.fit_on_texts(['<EOS>'])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# def createGeneratorData(df, tokenizer, max_len) :\n",
    "#     #Prepare training for normal model\n",
    "#     x = []\n",
    "#     y = []\n",
    "\n",
    "#     for seq in df['Input']:\n",
    "#         x_windows, y_windows = prepare_sentence(seq, max_len, tokenizer)\n",
    "#         x += x_windows\n",
    "#         y += y_windows\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)  # The word <PAD> does not constitute a class\n",
    "\n",
    "#     x.shape = [len(x), max_len + 1, 1]\n",
    "#     y.shape = [len(y), max_len + 1, 1]\n",
    "\n",
    "#     return x, y\n",
    "\n",
    "def prepare_sentence_classification(seq, maxlen, tokenizer):\n",
    "    # Pads seq and slides windows\n",
    "    seq = seq[:maxlen]\n",
    "#     seqX = np.append(seq, tokenizer.word_index['<eos>'])\n",
    "    seqX = np.append(tokenizer.word_index['<sos>'], seq)\n",
    "    seqX = np.append(seqX, tokenizer.word_index['<eos>'])\n",
    "\n",
    "    x= pad_sequences([seqX],\n",
    "        maxlen=maxlen+2,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def create_input_data(df, tokenizer, max_len):\n",
    "    #Prepare training for normal model\n",
    "    x=[]; y=[]\n",
    "    for index, row in df.iterrows():\n",
    "        x_windows = prepare_sentence_classification(row['Input'], max_len, tokenizer)\n",
    "#         y_labels = row['Label']\n",
    "        y_labels = row['Attack']\n",
    "#         x += x_windows\n",
    "        x.append(x_windows)\n",
    "        y.append(y_labels)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "#     x.shape = [len(x), max_len+2, 1]\n",
    "    x = np.reshape(x, (len(x), (max_len+2)))#, 1))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Hulk dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config['metadata']['artefact']\n",
    "# config['metadata']['artefact'] = 'artefact_seqlen200hash750'\n",
    "# config['metadata']['artefact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of normal data:  24000\n"
     ]
    }
   ],
   "source": [
    "    # # N1 data\n",
    "    # df_normal = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'N1.csv')\n",
    "## new normal = atk data\n",
    "df_normal = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_full.csv')\n",
    "print(\"size of normal data: \", len(df_normal))\n",
    "# df_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2<SEP>2'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_normal['Input'][99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "online = True\n",
    "counter = False #True # '''False=no countermeasure'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No countermeasure\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n",
      "size of full attack data:  2000\n"
     ]
    }
   ],
   "source": [
    "if counter is False:\n",
    "    print(\"No countermeasure\")\n",
    "    # Entire train atk set. \n",
    "    if online is True :\n",
    "        count = 0\n",
    "        df_attack_interval = []\n",
    "        while(True) :\n",
    "            if not os.path.isfile(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv') :\n",
    "                break\n",
    "            df_attack = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv')\n",
    "            df_attack_interval.append(df_attack)\n",
    "            count = count + 1\n",
    "    else :\n",
    "        df_attackfull = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_full.csv')\n",
    "\n",
    "\n",
    "    for x in df_attack_interval :\n",
    "        print(\"size of full attack data: \", len(x))\n",
    "\n",
    "else:\n",
    "    print(\"Use countermeasure\")\n",
    "    # # Online Q interval data (vary 1--3)\n",
    "    np.random.seed(2022)\n",
    "\n",
    "    if online is True :\n",
    "        count = 0\n",
    "    #     lengths = []\n",
    "        df_attack_interval = []\n",
    "\n",
    "        ## initial attack chunk A_0 (first 5 mins)\n",
    "        interval_range = 5\n",
    "        df_attack = pd.concat((pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(i) + '.csv') for i in range(interval_range)))\n",
    "        count = count + 1\n",
    "        df_attack_interval.append(df_attack)\n",
    "\n",
    "        while(True) :\n",
    "            if not os.path.isfile(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(interval_range) + '.csv'):\n",
    "                break\n",
    "\n",
    "            next_range = np.random.randint(low=1, high=4)\n",
    "            print(\"next range: \",next_range)\n",
    "            max_range = 11\n",
    "            if (interval_range+next_range) > max_range:\n",
    "                if (interval_range == max_range):\n",
    "                    next_range = 1\n",
    "                else:\n",
    "                    next_range = (max_range+1)-interval_range\n",
    "\n",
    "            df_attack = pd.concat((pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(i) + '.csv') for i in range(interval_range,interval_range+next_range)))\n",
    "            interval_range = interval_range+next_range\n",
    "\n",
    "            df_attack_interval.append(df_attack)\n",
    "            count = count + 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Entire train atk set. \n",
    "if online is True :\n",
    "    count = 0\n",
    "    df_attack_interval = []\n",
    "    while(True) :\n",
    "        if not os.path.isfile(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv') :\n",
    "            break\n",
    "        df_attack = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv')\n",
    "        df_attack['Attack'] = 1 #ADD\n",
    "        df_attack_interval.append(df_attack)\n",
    "        count = count + 1\n",
    "else :\n",
    "    df_attackfull = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_full.csv')\n",
    "\n",
    "\n",
    "for x in df_attack_interval :\n",
    "    print(\"size of full attack data: \", len(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# # Online Q interval data (vary 1--3)\n",
    "np.random.seed(2022)\n",
    "\n",
    "if online is True :\n",
    "    count = 0\n",
    "#     lengths = []\n",
    "    df_attack_interval = []\n",
    "\n",
    "    ## initial attack chunk A_0 (first 5 mins)\n",
    "    interval_range = 5\n",
    "    df_attack = pd.concat((pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(i) + '.csv') for i in range(interval_range)))\n",
    "    count = count + 1\n",
    "    df_attack_interval.append(df_attack)\n",
    "\n",
    "    while(True) :\n",
    "        if not os.path.isfile(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(interval_range) + '.csv'):\n",
    "            break\n",
    "\n",
    "        next_range = np.random.randint(low=1, high=4)\n",
    "        print(\"next range: \",next_range)\n",
    "        max_range = 11\n",
    "        if (interval_range+next_range) > max_range:\n",
    "            if (interval_range == max_range):\n",
    "                next_range = 1\n",
    "            else:\n",
    "                next_range = (max_range+1)-interval_range\n",
    "\n",
    "        df_attack = pd.concat((pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(i) + '.csv') for i in range(interval_range,interval_range+next_range)))\n",
    "        interval_range = interval_range+next_range\n",
    "        \n",
    "        df_attack_interval.append(df_attack)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n",
      "size of smaller attack data:  2000\n"
     ]
    }
   ],
   "source": [
    "# make attack ratio of train data if available.\n",
    "ratio = 0.5 #0.8 #0.2\n",
    "df_attack_interval2 = []\n",
    "for df_attackfull in df_attack_interval :\n",
    "    df_attacksmall = df_attackfull[-int(len(df_normal)/ratio - len(df_normal)):]\n",
    "    print(\"size of smaller attack data: \", len(df_attacksmall))\n",
    "    df_attack_interval2.append(df_attacksmall)\n",
    "df_attack_interval = df_attack_interval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n",
      "size of df_train:  2200\n"
     ]
    }
   ],
   "source": [
    "# add labels for evaluation\n",
    "df_attacksmall['Attack'] = 1\n",
    "df_normal['Attack'] = 0\n",
    "\n",
    "# Join and shuffle train attack and normal data\n",
    "#df_train = df_normal.append(df_attacksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "# df_train =df_normal # df_attacksmall #\n",
    "#print(\"size of df_train: \", len(df_train))\n",
    "\n",
    "df_normal['Attack'] = 0\n",
    "\n",
    "df_train_arr = []\n",
    "for df_attacksmall in df_attack_interval :\n",
    "    df_attacksmall['Attack'] = 1\n",
    "    df_normalsmall = df_normal.sample(n = int(len(df_attacksmall)*.1), replace=False, random_state=39)\n",
    "    df_train = df_normalsmall.append(df_attacksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "    df_train_arr.append(df_train)\n",
    "\n",
    "for x in df_train_arr :\n",
    "    print(\"size of df_train: \", len(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12263"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entire test set. \n",
    "dftest_atk = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'GENTEST.csv')\n",
    "len(dftest_atk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'84<SEP>17<SEP>92<SEP>5<SEP>17<SEP>138<SEP>32<SEP>133<SEP>127<SEP>39<SEP>34<SEP>145<SEP>165<SEP>86<SEP>131<SEP>208'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest_atk['Input'][66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24000\n",
      "size of smaller attack data:  12263\n",
      "size of df_test:  36263\n"
     ]
    }
   ],
   "source": [
    "# make test set \n",
    "# dfnorm_test = df_normal[-int(len(df_normal)*.5):] # last 50% of train norm as test\n",
    "dfnorm_test = df_normal\n",
    "print(len(dfnorm_test))\n",
    "\n",
    "ratio = 0.2 #0.2=80% atk , 0.5=50% atk\n",
    "# dftest_atksmall = dftest_atk[:(int(len(dfnorm_test)/ratio - len(dfnorm_test)))]\n",
    "dftest_atksmall = dftest_atk[-(int(len(dfnorm_test)/ratio - len(dfnorm_test))) : ]\n",
    "# add labels for evaluation\n",
    "dftest_atksmall['Attack'] = 1\n",
    "print(\"size of smaller attack data: \", len(dftest_atksmall))\n",
    "\n",
    "# join and shuffle train attack and normal data\n",
    "df_test = dfnorm_test.append(dftest_atksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "print(\"size of df_test: \", len(df_test))\n",
    "\n",
    "\n",
    "# # Saving test set\n",
    "# df_test.to_csv(config['metadata']['uniqueID'] + '/' + \n",
    "#                     config['metadata']['artefact'] + '/' + 'TEST.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==================== ADD ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_test:  12263\n"
     ]
    }
   ],
   "source": [
    "df_test = dftest_atksmall.sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "# resetting attack labels. attacker0='172.16.0.1'\n",
    "df_test['Attack'] = 0\n",
    "\n",
    "attacker1='999.16.30.0'; victim='192.168.10.50' # mimic attacker\n",
    "indexNames = df_test[(df_test['Source IP'] == attacker1)].index\n",
    "\n",
    "df_test['Attack'][indexNames] = 1\n",
    "# .drop(indexNames , inplace=True)\n",
    "# len(df_test_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(df_test['Attack'] == 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'TEST.csv')\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = config['SEQUENCELENGTH']\n",
    "\n",
    "tokenizer = getTokenizer(df_train_arr)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dftrain_embedded = df_train.copy()\n",
    "dftrain_embedded['Input'] = tokenizer.texts_to_sequences(df_train['Input'].values)\n",
    "# x_normal, y_normal = createGeneratorData(df_normal_embedded, tokenizer_normal, max_len)\n",
    "\n",
    "# Shuffle the data order\n",
    "dftrain_embedded_shuf = dftrain_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "\n",
    "# Extract inputs and labels\n",
    "X_train, y_train = create_input_data(dftrain_embedded_shuf, tokenizer, max_len)\n",
    "\n",
    "# Convert format for training  \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# np.unique(X_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dftest_embedded = df_test.copy()\n",
    "dftest_embedded['Input'] = tokenizer.texts_to_sequences(df_test['Input'].values)\n",
    "# x_normal, y_normal = createGeneratorData(df_normal_embedded, tokenizer_normal, max_len)\n",
    "\n",
    "# Shuffle the data order\n",
    "dftest_embedded_shuf = dftest_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "\n",
    "# Extract inputs and labels\n",
    "X_test, y_test = create_input_data(dftest_embedded_shuf, tokenizer, max_len)\n",
    "\n",
    "# Convert format for training  \n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_modelinputs(df, max_len, tokenizer):\n",
    "    df_embedded = df.copy()\n",
    "    df_embedded['Input'] = tokenizer.texts_to_sequences(df['Input'].values)\n",
    "    # Shuffle the data order\n",
    "    df_embedded_shuf = df_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "    # Extract inputs and labels\n",
    "    X_tmp, y_tmp = create_input_data(df_embedded_shuf, tokenizer, max_len)\n",
    "    # Convert format for training  \n",
    "    y_tmp = to_categorical(y_tmp, num_classes=2)\n",
    "    return X_tmp, y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 18) (24000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_norm, y_norm = prep_modelinputs(df_normal, max_len, tokenizer)\n",
    "print(X_norm.shape, y_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n",
      "(2200, 18) (2200, 2)\n"
     ]
    }
   ],
   "source": [
    "xtrainytrain_arr=[]\n",
    "for x in df_train_arr:\n",
    "    X_train, y_train = prep_modelinputs(x, max_len, tokenizer)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    xtrainytrain_arr.append((X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12263, 18) (12263, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = prep_modelinputs(df_test, max_len, tokenizer)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_train dataset:  (2200, 18)\n",
      "Number transactions y_train dataset:  (2200, 2)\n",
      "Number transactions X_test dataset:  (12263, 18)\n",
      "Number transactions y_test dataset:  (12263, 2)\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Training set, counts of label '0': 200\n",
      "Training set, counts of label '1': 2000\n",
      "Test set, counts of label '0': 2263\n",
      "Test set, counts of label '1': 10000\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "    print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "    \n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Training set, counts of label '0': {}\".format(int(np.sum(y_train, axis=0)[0])))\n",
    "    print(\"Training set, counts of label '1': {}\".format(int(np.sum(y_train, axis=0)[1])))\n",
    "print(\"Test set, counts of label '0': {}\".format(int(np.sum(y_test, axis=0)[0])))\n",
    "print(\"Test set, counts of label '1': {}\".format(int(np.sum(y_test, axis=0)[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  [253]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=[len(tokenizer.word_index)] \n",
    "input_emb_dim = config['MODELPARAMS']['INPUT_EMBED_DIM'] #512\n",
    "lstm_emb_dim = config['MODELPARAMS']['LSTM_DIM'] #300\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 512)           130048    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 18, 300)           975600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,827,450\n",
      "Trainable params: 1,827,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size[0] + 1, input_emb_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_emb_dim, return_sequences=True)) # dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(LSTM(lstm_emb_dim))\n",
    "# model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.adam(lr=0.005), \n",
    "              loss='binary_crossentropy', metrics=['acc']) #lr=0.005 config['MODELPARAMS']['LEARNING_RATE_P']\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=config['TRAININGPARAMS']['EPOCHS_P'], \n",
    "                      batch_size=config['TRAININGPARAMS']['BATCH_SIZE'], validation_split=0.2, \n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=7, \n",
    "                                               mode='auto', min_delta=0.0002)]) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "f = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.plot(epochs, acc, 'bo', label='Training acc')\n",
    "ax1.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "ax1.legend()\n",
    "ax1.title.set_text('Training and validation accuracy')\n",
    "\n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.title.set_text('Generated Synthetic Graph')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.plot(epochs, loss, 'bo', label='Training loss')\n",
    "ax2.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "ax2.legend()\n",
    "ax2.title.set_text('Training and validation loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Full classifier model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract true labels\n",
    "ytest_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accr = model.evaluate(X_test, y_test, batch_size=512)\n",
    "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_predscr = model.predict(X_test, batch_size=512, verbose=1)\n",
    "\n",
    "predpos = np.count_nonzero(y_predscr[:,1] > 0.5)\n",
    "pred_allpos = predpos / len(y_predscr)\n",
    "print('Percentage of Positives: {:0.4f}'.format(pred_allpos))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(ytest_true, y_pred))\n",
    "print('Recall:', recall_score(ytest_true, y_pred))\n",
    "print('Precision:', precision_score(ytest_true, y_pred))\n",
    "print('F1 score:', f1_score(ytest_true, y_pred))\n",
    "print('\\n clasification report:\\n', classification_report(ytest_true, y_pred))\n",
    "print('\\n confusion matrix:\\n',confusion_matrix(ytest_true, y_pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ytest_true, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "fpr = fp / (fp+tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fp_fullclassifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "    ytau_pred = (y_predscr[:,0] < tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "    fp_fullclassifier.append(fpr_tmp)\n",
    "    \n",
    "fp_fullclassifier.insert(0,0)\n",
    "fp_fullclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative classifier training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del iter_model\n",
    "# iter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 18, 512)           130048    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 18, 300)           975600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,827,450\n",
      "Trainable params: 1,827,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "iter_model = Sequential()\n",
    "iter_model.add(Embedding(vocab_size[0] + 1, input_emb_dim, input_length=X_train.shape[1]))\n",
    "iter_model.add(LSTM(lstm_emb_dim, return_sequences=True)) # dropout=0.2, recurrent_dropout=0.2))\n",
    "iter_model.add(LSTM(lstm_emb_dim))\n",
    "# model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "iter_model.add(Dense(2, activation='sigmoid'))\n",
    "iter_model.compile(optimizer=optimizers.adam(lr=0.005), \n",
    "              loss='binary_crossentropy', metrics=['acc']) #lr=0.005 config['MODELPARAMS']['LEARNING_RATE_P']\n",
    "\n",
    "print(iter_model.summary())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "keep_thres = 0.4 #0.4 # must be <=0.5\n",
    "count = 0\n",
    "\n",
    "#Number of iterations per interval. If want can change to array maybe?\n",
    "iterations = 5\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Training for interval\", count, \" \")\n",
    "    for _ in range(2):\n",
    "        iter_model.fit(X_train, y_train, epochs=2, #10, \n",
    "                          batch_size=config['TRAININGPARAMS']['ONLINE_BATCH_SIZE'], validation_split=0.2, \n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', patience=7, \n",
    "                                               mode='auto', min_delta=0.0002)]) #\n",
    "\n",
    "        y_predscr = iter_model.predict(X_train, batch_size=config['TRAININGPARAMS']['BATCH_SIZE'], verbose=1)\n",
    "    \n",
    "        # Sorting y_predscr and X_train\n",
    "        sorting = np.argsort(-1*y_predscr[:, 1]) # add (-1*) to argsort the second column in descending order \n",
    "        ypred_sorted = y_predscr[sorting] \n",
    "        Xtrain_sorted = X_train[sorting]\n",
    "    \n",
    "        # reduce Xtrain_sorted and ypred_sorted to keep_num size\n",
    "        print(len(y_predscr))\n",
    "        keep_num = int(len(y_predscr)*keep_thres)\n",
    "        print(keep_num)\n",
    "        Xtrain_atk = Xtrain_sorted[:keep_num,:]\n",
    "        ytrain_atk = np.vstack((np.zeros(keep_num), np.ones(keep_num))).T #.shape\n",
    "    \n",
    "        # randomly choose from X_norm to create new training set\n",
    "        Xtrain_norm = X_norm[np.random.choice(X_norm.shape[0], size=keep_num, replace=False), :]\n",
    "        X_train = np.concatenate((Xtrain_atk,Xtrain_norm), axis=0)\n",
    "        y_train = np.concatenate((ytrain_atk,y_norm[:len(Xtrain_norm)]), axis=0)\n",
    "    \n",
    "        # Shuffle new training set\n",
    "        shuffler = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffler]\n",
    "        y_train = y_train[shuffler]\n",
    "    \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for interval 0  \n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 4s 2ms/step - loss: 0.3887 - acc: 0.8889 - val_loss: 0.2808 - val_acc: 0.9159\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 627us/step - loss: 0.2957 - acc: 0.9034 - val_loss: 0.2789 - val_acc: 0.9159\n",
      "2200/2200 [==============================] - 0s 209us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 1  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 633us/step - loss: 0.3231 - acc: 0.9074 - val_loss: 0.2975 - val_acc: 0.9159\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 615us/step - loss: 0.2837 - acc: 0.9071 - val_loss: 0.3377 - val_acc: 0.9057\n",
      "2200/2200 [==============================] - 0s 58us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 2  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 604us/step - loss: 0.2735 - acc: 0.9082 - val_loss: 0.3438 - val_acc: 0.9136\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 640us/step - loss: 0.2370 - acc: 0.9159 - val_loss: 0.3725 - val_acc: 0.9045\n",
      "2200/2200 [==============================] - 0s 61us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 3  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 653us/step - loss: 0.2154 - acc: 0.9185 - val_loss: 0.3844 - val_acc: 0.8943\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 581us/step - loss: 0.1880 - acc: 0.9313 - val_loss: 0.3671 - val_acc: 0.8875\n",
      "2200/2200 [==============================] - 0s 57us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 4  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 631us/step - loss: 0.2204 - acc: 0.9085 - val_loss: 0.3923 - val_acc: 0.9080\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 624us/step - loss: 0.1764 - acc: 0.9352 - val_loss: 0.4487 - val_acc: 0.9114\n",
      "2200/2200 [==============================] - 0s 61us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 5  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 605us/step - loss: 0.1704 - acc: 0.9335 - val_loss: 0.5739 - val_acc: 0.8932\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 644us/step - loss: 0.1550 - acc: 0.9392 - val_loss: 0.4882 - val_acc: 0.8989\n",
      "2200/2200 [==============================] - 0s 67us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 6  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 686us/step - loss: 0.1693 - acc: 0.9358 - val_loss: 0.4282 - val_acc: 0.8977\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 656us/step - loss: 0.1249 - acc: 0.9577 - val_loss: 0.4826 - val_acc: 0.8909\n",
      "2200/2200 [==============================] - 0s 54us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 7  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 640us/step - loss: 0.1535 - acc: 0.9420 - val_loss: 0.5379 - val_acc: 0.9057\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 582us/step - loss: 0.1100 - acc: 0.9616 - val_loss: 0.6036 - val_acc: 0.8864\n",
      "2200/2200 [==============================] - 0s 59us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 8  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 616us/step - loss: 0.1226 - acc: 0.9545 - val_loss: 0.6536 - val_acc: 0.8932\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 598us/step - loss: 0.1036 - acc: 0.9693 - val_loss: 0.6657 - val_acc: 0.8977\n",
      "2200/2200 [==============================] - 0s 57us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 9  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 630us/step - loss: 0.1248 - acc: 0.9562 - val_loss: 0.5299 - val_acc: 0.8977\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 628us/step - loss: 0.1057 - acc: 0.9668 - val_loss: 0.5806 - val_acc: 0.8955\n",
      "2200/2200 [==============================] - 0s 57us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 10  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 643us/step - loss: 0.1220 - acc: 0.9568 - val_loss: 0.6290 - val_acc: 0.8830\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 626us/step - loss: 0.0998 - acc: 0.9676 - val_loss: 0.6916 - val_acc: 0.8977\n",
      "2200/2200 [==============================] - 0s 61us/step\n",
      "2200\n",
      "1100\n",
      "Training for interval 11  \n",
      "Train on 1760 samples, validate on 440 samples\n",
      "Epoch 1/2\n",
      "1760/1760 [==============================] - 1s 680us/step - loss: 0.1112 - acc: 0.9622 - val_loss: 0.5960 - val_acc: 0.9114\n",
      "Epoch 2/2\n",
      "1760/1760 [==============================] - 1s 641us/step - loss: 0.0899 - acc: 0.9710 - val_loss: 0.5828 - val_acc: 0.9068\n",
      "2200/2200 [==============================] - 0s 67us/step\n",
      "2200\n",
      "1100\n"
     ]
    }
   ],
   "source": [
    "keep_thres = 0.5 #0.4 #0.2 # must be <=0.5\n",
    "count = 0\n",
    "\n",
    "#Number of iterations per interval. If want can change to array maybe?\n",
    "iterations = 2 #5 #1 #\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Training for interval\", count, \" \")\n",
    "    for _ in range(1):\n",
    "        iter_model.fit(X_train, y_train, epochs=iterations, #10, \n",
    "                          batch_size=config['TRAININGPARAMS']['ONLINE_BATCH_SIZE'], validation_split=0.2, \n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', patience=7, \n",
    "                                               mode='auto', min_delta=0.0002)]) #\n",
    "\n",
    "        y_predscr = iter_model.predict(X_train, batch_size=config['TRAININGPARAMS']['BATCH_SIZE'], verbose=1)\n",
    "    \n",
    "        # Sorting y_predscr and X_train\n",
    "        sorting = np.argsort(-1*y_predscr[:, 1]) # add (-1*) to argsort the second column in descending order \n",
    "        ypred_sorted = y_predscr[sorting] \n",
    "        Xtrain_sorted = X_train[sorting]\n",
    "    \n",
    "        # reduce Xtrain_sorted and ypred_sorted to keep_num size\n",
    "        print(len(y_predscr))\n",
    "        keep_num = int(len(y_predscr)*keep_thres)\n",
    "        print(keep_num)\n",
    "        Xtrain_atk = Xtrain_sorted[:keep_num,:]\n",
    "        ytrain_atk = np.vstack((np.zeros(keep_num), np.ones(keep_num))).T #.shape\n",
    "    \n",
    "        # randomly choose from X_norm to create new training set\n",
    "        Xtrain_norm = X_norm[np.random.choice(X_norm.shape[0], size=keep_num, replace=False), :]\n",
    "        X_train = np.concatenate((Xtrain_atk,Xtrain_norm), axis=0)\n",
    "        y_train = np.concatenate((ytrain_atk,y_norm[:len(Xtrain_norm)]), axis=0)\n",
    "    \n",
    "        # Shuffle new training set\n",
    "        shuffler = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffler]\n",
    "        y_train = y_train[shuffler]\n",
    "    \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2200, 18)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keep_num = int(len(y_predscr)*keep_thres)\n",
    "# y_predscr[:keep_num,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorting = np.argsort(-1*y_predscr[:, 1]) # add (-1*) to argsort the second column in descending order \n",
    "ypred_sorted = y_predscr[sorting] #[:10]\n",
    "ypred_sorted"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xtrain_sorted = X_train[sorting]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ypred_sorted[:keep_num,:].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.vstack((np.zeros(keep_num), np.ones(keep_num))).T#.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xtrain_norm = X_norm[np.random.choice(X_norm.shape[0], size=keep_num, replace=False), :]\n",
    "X_train = np.concatenate((Xtrain_atk,Xtrain_norm), axis=0)\n",
    "X_train[:6]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.concatenate((ytrain_atk,y_norm[:len(Xtrain_norm)]), axis=0)\n",
    "y_train[:9]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "shuffler = np.random.permutation(len(X_train))\n",
    "X_train_shuffled = X_train[shuffler]\n",
    "X_train_shuffled\n",
    "y_train_shuffled = y_train[shuffler]\n",
    "y_train_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Iterative classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels\n",
    "ytest_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 53us/step\n",
      "Test set\n",
      "  Loss: 0.6466\n",
      "  Accuracy: 0.8004\n"
     ]
    }
   ],
   "source": [
    "accr = iter_model.evaluate(X_test, y_test, batch_size=512)\n",
    "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 54us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = iter_model.predict_classes(X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 51us/step\n",
      "Percentage of Positives: 0.9834\n"
     ]
    }
   ],
   "source": [
    "y_predscr = iter_model.predict(X_test, batch_size=512, verbose=1)\n",
    "\n",
    "predpos = np.count_nonzero(y_predscr[:,1] > 0.5)\n",
    "pred_allpos = predpos / len(y_predscr)\n",
    "print('Percentage of Positives: {:0.4f}'.format(pred_allpos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8004566582402348\n",
      "Recall: 0.9806\n",
      "Precision: 0.8131685877767643\n",
      "F1 score: 0.8890702207715673\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.05      0.00      0.01      2263\n",
      "           1       0.81      0.98      0.89     10000\n",
      "\n",
      "    accuracy                           0.80     12263\n",
      "   macro avg       0.43      0.49      0.45     12263\n",
      "weighted avg       0.67      0.80      0.73     12263\n",
      "\n",
      "\n",
      " confusion matrix:\n",
      " [[  10 2253]\n",
      " [ 194 9806]]\n",
      "10 2253 194 9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9955810870525851"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr = 0.005\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(ytest_true, y_pred))\n",
    "print('Recall:', recall_score(ytest_true, y_pred))\n",
    "print('Precision:', precision_score(ytest_true, y_pred))\n",
    "print('F1 score:', f1_score(ytest_true, y_pred))\n",
    "print('\\n clasification report:\\n', classification_report(ytest_true, y_pred))\n",
    "print('\\n confusion matrix:\\n',confusion_matrix(ytest_true, y_pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ytest_true, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "fpr = fp / (fp+tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Iterative classifier\n",
    "Accuracy: 0.8177498467198038\n",
    "Recall: 0.6496014714898835\n",
    "Precision: 0.9787528868360277\n",
    "F1 score: 0.7809102634973282\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.74      0.99      0.84      3262\n",
    "           1       0.98      0.65      0.78      3262\n",
    "\n",
    "    accuracy                           0.82      6524\n",
    "   macro avg       0.86      0.82      0.81      6524\n",
    "weighted avg       0.86      0.82      0.81      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[3216   46]\n",
    " [1143 2119]]\n",
    "3216 46 1143 2119\n",
    "\n",
    "0.014101778050275904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.9770216526734423,\n",
       " 0.9946973044631021,\n",
       " 0.9951391957578436,\n",
       " 0.9955810870525851,\n",
       " 0.9955810870525851,\n",
       " 0.9960229783473266,\n",
       " 0.996464869642068,\n",
       " 0.9977905435262925,\n",
       " 0.999116217410517,\n",
       " 1.0]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_classifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "    ytau_pred = (y_predscr[:,0] < tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "    fp_classifier.append(fpr_tmp)\n",
    "    \n",
    "fp_classifier.insert(0,0)\n",
    "fp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.001, 0.0083, 0.0134, 0.0162, 0.0194, 0.0226, 0.0264, 0.0317, 0.0412, 1.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False negative rate: FNR=1TPR or FNR=FN/(TP+FN) \n",
    "fn_classifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "#     ytau_pred = (y_predscr[:,0] < (1-tau_threshold)).astype(int)\n",
    "    ytau_pred = (y_predscr[:,1] > tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "\n",
    "#     fnr_tmp = 1 - fpr_tmp\n",
    "    fnr_tmp = fn_tmp / (tp_tmp+fn_tmp)\n",
    "    fn_classifier.append(fnr_tmp)\n",
    "    \n",
    "fn_classifier.insert(0,0)\n",
    "fn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = y_predscr[:,1] # classifier atk scores\n",
    "# def showscore(scores):\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "print(len(scores))\n",
    "plt.hist(scores, bins = 10, alpha=0.5, label='Pred Scores') \n",
    "#     plt.hist(scoretype[scores][scoretype['Attack'] == 1], bins = 10, alpha=0.5, label='attacker')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "    \n",
    "# showscore(y_predscr) # P scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.003\n",
    "\n",
    "Accuracy: 0.8068669527896996\n",
    "Recall: 0.6226241569589209\n",
    "Precision: 0.9859223300970874\n",
    "F1 score: 0.7632468996617813\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.99      0.84      3262\n",
    "           1       0.99      0.62      0.76      3262\n",
    "\n",
    "    accuracy                           0.81      6524\n",
    "   macro avg       0.86      0.81      0.80      6524\n",
    "weighted avg       0.86      0.81      0.80      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[3233   29]\n",
    " [1231 2031]]\n",
    "3233 29 1231 2031\n",
    "\n",
    "0.008890251379521767"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.0005\n",
    "Accuracy: 0.76931330472103\n",
    "Recall: 0.6489883507050889\n",
    "Precision: 0.8546628986677433\n",
    "F1 score: 0.7377591914967764\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.89      0.79      3262\n",
    "           1       0.85      0.65      0.74      3262\n",
    "\n",
    "    accuracy                           0.77      6524\n",
    "   macro avg       0.79      0.77      0.77      6524\n",
    "weighted avg       0.79      0.77      0.77      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[2902  360]\n",
    " [1145 2117]]\n",
    "2902 360 1145 2117\n",
    "\n",
    "0.11036174126302882"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.001\n",
    "Accuracy: 0.7967504598405886\n",
    "Recall: 0.6152667075413857\n",
    "Precision: 0.965832531280077\n",
    "F1 score: 0.751685393258427\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.98      0.83      3262\n",
    "           1       0.97      0.62      0.75      3262\n",
    "\n",
    "    accuracy                           0.80      6524\n",
    "   macro avg       0.84      0.80      0.79      6524\n",
    "weighted avg       0.84      0.80      0.79      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[3191   71]\n",
    " [1255 2007]]\n",
    "3191 71 1255 2007\n",
    "\n",
    "0.021765787860208462"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "TP, FP, TN, FN = perf_measure(ytest_true, y_pred)\n",
    "FPR = FP / (FP+TN)\n",
    "FPR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddos",
   "language": "python",
   "name": "ddos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

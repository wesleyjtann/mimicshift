{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import sys\n",
    "import yaml\n",
    "\n",
    "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Flatten, TimeDistributed, Bidirectional\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.utils import plot_model\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import concatenate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from sklearn import preprocessing\n",
    "from copy import deepcopy\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import hashlib\n",
    "import time\n",
    "import os\n",
    "from datagenerator import DataGenerator\n",
    "\n",
    "# add\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #\"1\"  # specify which GPU(s) to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Disable randomization\n",
    "import random\n",
    "seed_value=2020 #10 #\n",
    "os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "# add\n",
    "config = tf.ConfigProto() \n",
    "config.gpu_options.allow_growth = True \n",
    "# # sess = tf.Session(config=config) \n",
    "sess = tf.compat.v1.Session(config=config) \n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/ipykernel_launcher.py:4: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "def loadConfig():\n",
    "#     with open('/home/wesleyjtann/ddos_research/test/light_config_b.yaml', \"r\") as ymlfile:\n",
    "    with open('./light_config.yaml', \"r\") as ymlfile: #GE, hulk, slowhttp, slowloris, heartbleed\n",
    "        cfg = yaml.load(ymlfile)\n",
    "    return cfg\n",
    "\n",
    "config = loadConfig()\n",
    "# artefact = 'artefact' #'artefact_b' #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(seq, maxlen, tokenizer):\n",
    "    # Pads seq and slides windows\n",
    "    seq = seq[:maxlen]\n",
    "    seqX = np.append(tokenizer.word_index['<sos>'], seq)\n",
    "    seqY = np.append(seq, tokenizer.word_index['<eos>'])\n",
    "\n",
    "    x= pad_sequences([seqX],\n",
    "        maxlen=maxlen+1,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    y= pad_sequences([seqY],\n",
    "        maxlen=maxlen+1,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    return [x], [y]\n",
    "\n",
    "\n",
    "def getTokenizer(df_arr) :\n",
    "    ### Dictionary for Normal ###\n",
    "    tokenizer = Tokenizer(filters='', split='<sep>', oov_token='<OTHERS>' ,lower=True)\n",
    "    \n",
    "    for df in df_arr :\n",
    "        tokenizer.fit_on_texts(df['Input'].values)\n",
    "\n",
    "    tokenizer.fit_on_texts(['<SOS>'])\n",
    "    tokenizer.fit_on_texts(['<EOS>'])\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# def createGeneratorData(df, tokenizer, max_len) :\n",
    "#     #Prepare training for normal model\n",
    "#     x = []\n",
    "#     y = []\n",
    "\n",
    "#     for seq in df['Input']:\n",
    "#         x_windows, y_windows = prepare_sentence(seq, max_len, tokenizer)\n",
    "#         x += x_windows\n",
    "#         y += y_windows\n",
    "#     x = np.array(x)\n",
    "#     y = np.array(y)  # The word <PAD> does not constitute a class\n",
    "\n",
    "#     x.shape = [len(x), max_len + 1, 1]\n",
    "#     y.shape = [len(y), max_len + 1, 1]\n",
    "\n",
    "#     return x, y\n",
    "\n",
    "def prepare_sentence_classification(seq, maxlen, tokenizer):\n",
    "    # Pads seq and slides windows\n",
    "    seq = seq[:maxlen]\n",
    "#     seqX = np.append(seq, tokenizer.word_index['<eos>'])\n",
    "    seqX = np.append(tokenizer.word_index['<sos>'], seq)\n",
    "    seqX = np.append(seqX, tokenizer.word_index['<eos>'])\n",
    "\n",
    "    x= pad_sequences([seqX],\n",
    "        maxlen=maxlen+2,\n",
    "        padding='post')[0]  # Pads before each sequence\n",
    "\n",
    "    return [x]\n",
    "\n",
    "\n",
    "def create_input_data(df, tokenizer, max_len):\n",
    "    #Prepare training for normal model\n",
    "    x=[]; y=[]\n",
    "    for index, row in df.iterrows():\n",
    "        x_windows = prepare_sentence_classification(row['Input'], max_len, tokenizer)\n",
    "#         y_labels = row['Label']\n",
    "        y_labels = row['Attack']\n",
    "#         x += x_windows\n",
    "        x.append(x_windows)\n",
    "        y.append(y_labels)\n",
    "        \n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "#     x.shape = [len(x), max_len+2, 1]\n",
    "    x = np.reshape(x, (len(x), (max_len+2)))#, 1))\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CAIDA07 dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "config['metadata']['artefact']\n",
    "# config['metadata']['artefact'] = 'artefact_seqlen200hash750'\n",
    "# config['metadata']['artefact']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ids_test3/artefact'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config['metadata']['uniqueID'] + '/' + config['metadata']['artefact']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of normal data:  24000\n"
     ]
    }
   ],
   "source": [
    "# # N1 data\n",
    "# df_normal = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'N1.csv')\n",
    "## new normal = atk data\n",
    "df_normal = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_full.csv')\n",
    "print(\"size of normal data: \", len(df_normal))\n",
    "# df_normal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# online = True\n",
    "online = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of full attack data:  24000\n"
     ]
    }
   ],
   "source": [
    "# Entire train atk set. \n",
    "df_attack_interval = []\n",
    "\n",
    "if online is True :\n",
    "    count = 0\n",
    "#     df_attack_interval = []\n",
    "    while(True) :\n",
    "        if not os.path.isfile(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv') :\n",
    "            break\n",
    "        df_attack = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_' + str(count) + '.csv')\n",
    "        df_attack_interval.append(df_attack)\n",
    "        count = count + 1\n",
    "else :\n",
    "    df_attackfull = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'A1_full.csv')\n",
    "    df_attackfull['Attack'] = 1\n",
    "    df_attack_interval.append(df_attackfull)\n",
    "\n",
    "\n",
    "for x in df_attack_interval :\n",
    "    print(\"size of full attack data: \", len(x))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_attackfull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of smaller attack data:  24000\n"
     ]
    }
   ],
   "source": [
    "# make attack ratio of train data if available.\n",
    "ratio = 0.5 #0.8 #0.2\n",
    "df_attack_interval2 = []\n",
    "for df_attackfull in df_attack_interval :\n",
    "    df_attacksmall = df_attackfull[-int(len(df_normal)/ratio - len(df_normal)):]\n",
    "    print(\"size of smaller attack data: \", len(df_attacksmall))\n",
    "    df_attack_interval2.append(df_attacksmall)\n",
    "df_attack_interval = df_attack_interval2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_train:  33600\n"
     ]
    }
   ],
   "source": [
    "# add labels for evaluation\n",
    "df_attacksmall['Attack'] = 1\n",
    "df_normal['Attack'] = 0\n",
    "\n",
    "# Join and shuffle train attack and normal data\n",
    "#df_train = df_normal.append(df_attacksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "# df_train =df_normal # df_attacksmall #\n",
    "#print(\"size of df_train: \", len(df_train))\n",
    "\n",
    "df_normal['Attack'] = 0\n",
    "\n",
    "df_train_arr = []\n",
    "for df_attacksmall in df_attack_interval :\n",
    "    df_attacksmall['Attack'] = 1\n",
    "    df_normalsmall = df_normal.sample(n = int(len(df_attacksmall)*.4), replace=False, random_state=39)\n",
    "    df_train = df_normalsmall.append(df_attacksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "    df_train_arr.append(df_train)\n",
    "\n",
    "for x in df_train_arr :\n",
    "    print(\"size of df_train: \", len(x))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_test:  12263\n"
     ]
    }
   ],
   "source": [
    "# GENTEST\n",
    "df_test = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'GENTEST.csv')\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Entire test set. \n",
    "dftest_atk = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'GENTEST.csv')\n",
    "len(dftest_atk)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "dftest_atk = dftest_atk.sample(frac=1, replace=False, random_state=39)\n",
    "sum(dftest_atk['Attack'] == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of df_test:  12263\n"
     ]
    }
   ],
   "source": [
    "# df_test = dftest_atk.sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "df_test = df_test.sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2263\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(sum(df_test['Attack'] == 0))\n",
    "sum(df_test['Attack'] == 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# make test set \n",
    "# dfnorm_test = df_normal[-int(len(df_normal)*.5):] # last 50% of train norm as test\n",
    "dfnorm_test = df_normal\n",
    "print(len(dfnorm_test))\n",
    "\n",
    "ratio = 0.2 #0.2=80% atk , 0.5=50% atk\n",
    "# dftest_atksmall = dftest_atk[:(int(len(dfnorm_test)/ratio - len(dfnorm_test)))]\n",
    "dftest_atksmall = dftest_atk[-(int(len(dfnorm_test)/ratio - len(dfnorm_test))) : ]\n",
    "# add labels for evaluation\n",
    "dftest_atksmall['Attack'] = 1\n",
    "print(\"size of smaller attack data: \", len(dftest_atksmall))\n",
    "\n",
    "# join and shuffle train attack and normal data\n",
    "df_test = dfnorm_test.append(dftest_atksmall).sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "print(\"size of df_test: \", len(df_test))\n",
    "\n",
    "# # Saving test set\n",
    "# df_test.to_csv(config['metadata']['uniqueID'] + '/' + \n",
    "#                     config['metadata']['artefact'] + '/' + 'TEST.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ==================== ADD ===================="
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test = dftest_atksmall.sample(frac=1, replace=False, random_state=39).reset_index(drop=True)\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# resetting attack labels. attacker0='172.16.0.1'\n",
    "df_test['Attack'] = 0\n",
    "\n",
    "attacker1='999.16.30.0'; victim='192.168.10.50' # mimic attacker\n",
    "indexNames = df_test[(df_test['Source IP'] == attacker1)].index\n",
    "\n",
    "df_test['Attack'][indexNames] = 1\n",
    "# .drop(indexNames , inplace=True)\n",
    "# len(df_test_original)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sum(df_test['Attack'] == 1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test = pd.read_csv(config['metadata']['uniqueID'] + '/' + config['metadata']['artefact'] + '/' + 'TEST.csv')\n",
    "print(\"size of df_test: \", len(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = config['SEQUENCELENGTH']\n",
    "\n",
    "tokenizer = getTokenizer(df_train_arr)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dftrain_embedded = df_train.copy()\n",
    "dftrain_embedded['Input'] = tokenizer.texts_to_sequences(df_train['Input'].values)\n",
    "# x_normal, y_normal = createGeneratorData(df_normal_embedded, tokenizer_normal, max_len)\n",
    "\n",
    "# Shuffle the data order\n",
    "dftrain_embedded_shuf = dftrain_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "\n",
    "# Extract inputs and labels\n",
    "X_train, y_train = create_input_data(dftrain_embedded_shuf, tokenizer, max_len)\n",
    "\n",
    "# Convert format for training  \n",
    "y_train = to_categorical(y_train, num_classes=2)\n",
    "\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# np.unique(X_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dftest_embedded = df_test.copy()\n",
    "dftest_embedded['Input'] = tokenizer.texts_to_sequences(df_test['Input'].values)\n",
    "# x_normal, y_normal = createGeneratorData(df_normal_embedded, tokenizer_normal, max_len)\n",
    "\n",
    "# Shuffle the data order\n",
    "dftest_embedded_shuf = dftest_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "\n",
    "# Extract inputs and labels\n",
    "X_test, y_test = create_input_data(dftest_embedded_shuf, tokenizer, max_len)\n",
    "\n",
    "# Convert format for training  \n",
    "y_test = to_categorical(y_test, num_classes=2)\n",
    "\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_modelinputs(df, max_len, tokenizer):\n",
    "    df_embedded = df.copy()\n",
    "    df_embedded['Input'] = tokenizer.texts_to_sequences(df['Input'].values)\n",
    "    # Shuffle the data order\n",
    "    df_embedded_shuf = df_embedded.sample(frac=1, random_state=42, replace=False)\n",
    "    # Extract inputs and labels\n",
    "    X_tmp, y_tmp = create_input_data(df_embedded_shuf, tokenizer, max_len)\n",
    "    # Convert format for training  \n",
    "    y_tmp = to_categorical(y_tmp, num_classes=2)\n",
    "    return X_tmp, y_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24000, 18) (24000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_norm, y_norm = prep_modelinputs(df_normal, max_len, tokenizer)\n",
    "print(X_norm.shape, y_norm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33600, 18) (33600, 2)\n"
     ]
    }
   ],
   "source": [
    "xtrainytrain_arr=[]\n",
    "for x in df_train_arr:\n",
    "    X_train, y_train = prep_modelinputs(x, max_len, tokenizer)\n",
    "    print(X_train.shape, y_train.shape)\n",
    "    xtrainytrain_arr.append((X_train, y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12263, 18) (12263, 2)\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = prep_modelinputs(df_test, max_len, tokenizer)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number transactions X_train dataset:  (33600, 18)\n",
      "Number transactions y_train dataset:  (33600, 2)\n",
      "Number transactions X_test dataset:  (12263, 18)\n",
      "Number transactions y_test dataset:  (12263, 2)\n",
      "Training set, counts of label '0': 9600\n",
      "Training set, counts of label '1': 24000\n",
      "Test set, counts of label '0': 2263\n",
      "Test set, counts of label '1': 10000\n"
     ]
    }
   ],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(X, y_cat, test_size=0.2, random_state=42)\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Number transactions X_train dataset: \", X_train.shape)\n",
    "    print(\"Number transactions y_train dataset: \", y_train.shape)\n",
    "    \n",
    "print(\"Number transactions X_test dataset: \", X_test.shape)\n",
    "print(\"Number transactions y_test dataset: \", y_test.shape)\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Training set, counts of label '0': {}\".format(int(np.sum(y_train, axis=0)[0])))\n",
    "    print(\"Training set, counts of label '1': {}\".format(int(np.sum(y_train, axis=0)[1])))\n",
    "print(\"Test set, counts of label '0': {}\".format(int(np.sum(y_test, axis=0)[0])))\n",
    "print(\"Test set, counts of label '1': {}\".format(int(np.sum(y_test, axis=0)[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:  [253]\n"
     ]
    }
   ],
   "source": [
    "vocab_size=[len(tokenizer.word_index)] \n",
    "input_emb_dim = config['MODELPARAMS']['INPUT_EMBED_DIM'] #512\n",
    "lstm_emb_dim = config['MODELPARAMS']['LSTM_DIM'] #300\n",
    "print(\"vocab_size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/wesleyjtann/miniconda3/envs/ddos/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 18, 512)           130048    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 18, 300)           975600    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,827,450\n",
      "Trainable params: 1,827,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size[0] + 1, input_emb_dim, input_length=X_train.shape[1]))\n",
    "model.add(LSTM(lstm_emb_dim, return_sequences=True)) # dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(LSTM(lstm_emb_dim))\n",
    "# model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "model.compile(optimizer=optimizers.adam(lr=0.005), \n",
    "              loss='binary_crossentropy', metrics=['acc']) #lr=0.005 config['MODELPARAMS']['LEARNING_RATE_P']\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "history = model.fit(X_train, y_train, epochs=config['TRAININGPARAMS']['EPOCHS_P'], \n",
    "                      batch_size=config['TRAININGPARAMS']['BATCH_SIZE'], validation_split=0.2, \n",
    "                      callbacks=[EarlyStopping(monitor='val_loss', patience=7, \n",
    "                                               mode='auto', min_delta=0.0002)]) #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "f = plt.figure(figsize=(15,5))\n",
    "\n",
    "ax1 = f.add_subplot(121)\n",
    "ax1.set_xlabel('Epochs')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.plot(epochs, acc, 'bo', label='Training acc')\n",
    "ax1.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "ax1.legend()\n",
    "ax1.title.set_text('Training and validation accuracy')\n",
    "\n",
    "ax2 = f.add_subplot(122)\n",
    "ax2.title.set_text('Generated Synthetic Graph')\n",
    "ax2.set_xlabel('Epochs')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.plot(epochs, loss, 'bo', label='Training loss')\n",
    "ax2.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "ax2.legend()\n",
    "ax2.title.set_text('Training and validation loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Full classifier model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Extract true labels\n",
    "ytest_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "accr = model.evaluate(X_test, y_test, batch_size=512)\n",
    "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_pred = model.predict_classes(X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "y_predscr = model.predict(X_test, batch_size=512, verbose=1)\n",
    "\n",
    "predpos = np.count_nonzero(y_predscr[:,1] > 0.5)\n",
    "pred_allpos = predpos / len(y_predscr)\n",
    "print('Percentage of Positives: {:0.4f}'.format(pred_allpos))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(ytest_true, y_pred))\n",
    "print('Recall:', recall_score(ytest_true, y_pred))\n",
    "print('Precision:', precision_score(ytest_true, y_pred))\n",
    "print('F1 score:', f1_score(ytest_true, y_pred))\n",
    "print('\\n clasification report:\\n', classification_report(ytest_true, y_pred))\n",
    "print('\\n confusion matrix:\\n',confusion_matrix(ytest_true, y_pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ytest_true, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "fpr = fp / (fp+tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fp_fullclassifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "    ytau_pred = (y_predscr[:,0] < tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "    fp_fullclassifier.append(fpr_tmp)\n",
    "    \n",
    "fp_fullclassifier.insert(0,0)\n",
    "fp_fullclassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative classifier training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "del iter_model\n",
    "# iter_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 18, 512)           130048    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 18, 300)           975600    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 602       \n",
      "=================================================================\n",
      "Total params: 1,827,450\n",
      "Trainable params: 1,827,450\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "iter_model = Sequential()\n",
    "iter_model.add(Embedding(vocab_size[0] + 1, input_emb_dim, input_length=X_train.shape[1]))\n",
    "iter_model.add(LSTM(lstm_emb_dim, return_sequences=True)) # dropout=0.2, recurrent_dropout=0.2))\n",
    "iter_model.add(LSTM(lstm_emb_dim))\n",
    "# model.add(TimeDistributed(Dense(2, activation='sigmoid')))\n",
    "iter_model.add(Dense(2, activation='sigmoid'))\n",
    "iter_model.compile(optimizer=optimizers.adam(lr=0.005), \n",
    "              loss='binary_crossentropy', metrics=['acc']) #lr=0.005 config['MODELPARAMS']['LEARNING_RATE_P']\n",
    "\n",
    "print(iter_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for interval 0  \n",
      "Train on 26880 samples, validate on 6720 samples\n",
      "Epoch 1/5\n",
      "26880/26880 [==============================] - 16s 613us/step - loss: 0.6999 - acc: 0.7022 - val_loss: 0.5976 - val_acc: 0.7165\n",
      "Epoch 2/5\n",
      "26880/26880 [==============================] - 16s 609us/step - loss: 0.5994 - acc: 0.7137 - val_loss: 0.5979 - val_acc: 0.7165\n",
      "Epoch 3/5\n",
      "26880/26880 [==============================] - 16s 586us/step - loss: 0.5998 - acc: 0.7137 - val_loss: 0.5999 - val_acc: 0.7165\n",
      "Epoch 4/5\n",
      "26880/26880 [==============================] - 16s 590us/step - loss: 0.5990 - acc: 0.7137 - val_loss: 0.5993 - val_acc: 0.7165\n",
      "Epoch 5/5\n",
      "26880/26880 [==============================] - 16s 594us/step - loss: 0.5985 - acc: 0.7137 - val_loss: 0.6003 - val_acc: 0.7165\n",
      "33600/33600 [==============================] - 2s 50us/step\n",
      "33600\n",
      "13440\n",
      "Train on 21504 samples, validate on 5376 samples\n",
      "Epoch 1/5\n",
      "21504/21504 [==============================] - 12s 581us/step - loss: 0.5855 - acc: 0.6793 - val_loss: 0.5470 - val_acc: 0.7090\n",
      "Epoch 2/5\n",
      "21504/21504 [==============================] - 13s 598us/step - loss: 0.5402 - acc: 0.7132 - val_loss: 0.5503 - val_acc: 0.7041\n",
      "Epoch 3/5\n",
      "21504/21504 [==============================] - 13s 602us/step - loss: 0.5294 - acc: 0.7208 - val_loss: 0.5378 - val_acc: 0.7121\n",
      "Epoch 4/5\n",
      "21504/21504 [==============================] - 13s 600us/step - loss: 0.5215 - acc: 0.7277 - val_loss: 0.5416 - val_acc: 0.7113\n",
      "Epoch 5/5\n",
      "21504/21504 [==============================] - 13s 607us/step - loss: 0.5155 - acc: 0.7293 - val_loss: 0.5307 - val_acc: 0.7171\n",
      "26880/26880 [==============================] - 1s 53us/step\n",
      "26880\n",
      "10752\n",
      "Train on 17203 samples, validate on 4301 samples\n",
      "Epoch 1/5\n",
      "17203/17203 [==============================] - 10s 608us/step - loss: 0.3995 - acc: 0.8228 - val_loss: 0.3850 - val_acc: 0.8339\n",
      "Epoch 2/5\n",
      "17203/17203 [==============================] - 11s 622us/step - loss: 0.3788 - acc: 0.8356 - val_loss: 0.3902 - val_acc: 0.8288\n",
      "Epoch 3/5\n",
      "17203/17203 [==============================] - 11s 616us/step - loss: 0.3693 - acc: 0.8417 - val_loss: 0.3888 - val_acc: 0.8350\n",
      "Epoch 4/5\n",
      "17203/17203 [==============================] - 10s 593us/step - loss: 0.3656 - acc: 0.8422 - val_loss: 0.3870 - val_acc: 0.8314\n",
      "Epoch 5/5\n",
      "17203/17203 [==============================] - 10s 610us/step - loss: 0.3605 - acc: 0.8455 - val_loss: 0.3818 - val_acc: 0.8353\n",
      "21504/21504 [==============================] - 1s 49us/step\n",
      "21504\n",
      "8601\n",
      "Train on 13761 samples, validate on 3441 samples\n",
      "Epoch 1/5\n",
      "13761/13761 [==============================] - 8s 587us/step - loss: 0.3138 - acc: 0.8733 - val_loss: 0.3287 - val_acc: 0.8653\n",
      "Epoch 2/5\n",
      "13761/13761 [==============================] - 8s 586us/step - loss: 0.2989 - acc: 0.8835 - val_loss: 0.3258 - val_acc: 0.8688\n",
      "Epoch 3/5\n",
      "13761/13761 [==============================] - 8s 599us/step - loss: 0.2888 - acc: 0.8903 - val_loss: 0.3199 - val_acc: 0.8772\n",
      "Epoch 4/5\n",
      "13761/13761 [==============================] - 8s 596us/step - loss: 0.2779 - acc: 0.8947 - val_loss: 0.3326 - val_acc: 0.8746\n",
      "Epoch 5/5\n",
      "13761/13761 [==============================] - 8s 606us/step - loss: 0.2749 - acc: 0.8961 - val_loss: 0.3279 - val_acc: 0.8705\n",
      "17202/17202 [==============================] - 1s 48us/step\n",
      "17202\n",
      "6880\n",
      "Train on 11008 samples, validate on 2752 samples\n",
      "Epoch 1/5\n",
      "11008/11008 [==============================] - 7s 603us/step - loss: 0.2612 - acc: 0.9031 - val_loss: 0.2764 - val_acc: 0.8941\n",
      "Epoch 2/5\n",
      "11008/11008 [==============================] - 7s 615us/step - loss: 0.2399 - acc: 0.9139 - val_loss: 0.2908 - val_acc: 0.8917\n",
      "Epoch 3/5\n",
      "11008/11008 [==============================] - 7s 594us/step - loss: 0.2318 - acc: 0.9172 - val_loss: 0.2930 - val_acc: 0.8859\n",
      "Epoch 4/5\n",
      "11008/11008 [==============================] - 7s 600us/step - loss: 0.2306 - acc: 0.9199 - val_loss: 0.2972 - val_acc: 0.8848\n",
      "Epoch 5/5\n",
      "11008/11008 [==============================] - 7s 628us/step - loss: 0.2224 - acc: 0.9223 - val_loss: 0.2928 - val_acc: 0.8939\n",
      "13760/13760 [==============================] - 1s 50us/step\n",
      "13760\n",
      "5504\n"
     ]
    }
   ],
   "source": [
    "keep_thres = 0.4 #0.4 # must be <=0.5\n",
    "count = 0\n",
    "\n",
    "#Number of iterations per interval. If want can change to array maybe?\n",
    "iterations = 5 #1 ##2\n",
    "n_epochs = 5 #10 #2 #\n",
    "\n",
    "for (X_train, y_train) in xtrainytrain_arr :\n",
    "    print(\"Training for interval\", count, \" \")\n",
    "    for _ in range(iterations): #2):\n",
    "        iter_model.fit(X_train, y_train, epochs=n_epochs, \n",
    "                          batch_size=config['TRAININGPARAMS']['ONLINE_BATCH_SIZE'], validation_split=0.2, \n",
    "                          callbacks=[EarlyStopping(monitor='val_loss', patience=7, \n",
    "                                               mode='auto', min_delta=0.0002)]) #\n",
    "\n",
    "        y_predscr = iter_model.predict(X_train, batch_size=config['TRAININGPARAMS']['BATCH_SIZE'], verbose=1)\n",
    "    \n",
    "        # Sorting y_predscr and X_train\n",
    "        sorting = np.argsort(-1*y_predscr[:, 1]) # add (-1*) to argsort the second column in descending order \n",
    "        ypred_sorted = y_predscr[sorting] \n",
    "        Xtrain_sorted = X_train[sorting]\n",
    "    \n",
    "        # reduce Xtrain_sorted and ypred_sorted to keep_num size\n",
    "        print(len(y_predscr))\n",
    "        keep_num = int(len(y_predscr)*keep_thres)\n",
    "        print(keep_num)\n",
    "        Xtrain_atk = Xtrain_sorted[:keep_num,:]\n",
    "        ytrain_atk = np.vstack((np.zeros(keep_num), np.ones(keep_num))).T #.shape\n",
    "    \n",
    "        # randomly choose from X_norm to create new training set\n",
    "        Xtrain_norm = X_norm[np.random.choice(X_norm.shape[0], size=keep_num, replace=False), :]\n",
    "        X_train = np.concatenate((Xtrain_atk,Xtrain_norm), axis=0)\n",
    "        y_train = np.concatenate((ytrain_atk,y_norm[:len(Xtrain_norm)]), axis=0)\n",
    "    \n",
    "        # Shuffle new training set\n",
    "        shuffler = np.random.permutation(len(X_train))\n",
    "        X_train = X_train[shuffler]\n",
    "        y_train = y_train[shuffler]\n",
    "    \n",
    "    count = count + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11008, 18)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keep_num = int(len(y_predscr)*keep_thres)\n",
    "# y_predscr[:keep_num,1]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "sorting = np.argsort(-1*y_predscr[:, 1]) # add (-1*) to argsort the second column in descending order \n",
    "ypred_sorted = y_predscr[sorting] #[:10]\n",
    "ypred_sorted"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xtrain_sorted = X_train[sorting]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ypred_sorted[:keep_num,:].shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.vstack((np.zeros(keep_num), np.ones(keep_num))).T#.shape"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Xtrain_norm = X_norm[np.random.choice(X_norm.shape[0], size=keep_num, replace=False), :]\n",
    "X_train = np.concatenate((Xtrain_atk,Xtrain_norm), axis=0)\n",
    "X_train[:6]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "y_train = np.concatenate((ytrain_atk,y_norm[:len(Xtrain_norm)]), axis=0)\n",
    "y_train[:9]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "shuffler = np.random.permutation(len(X_train))\n",
    "X_train_shuffled = X_train[shuffler]\n",
    "X_train_shuffled\n",
    "y_train_shuffled = y_train[shuffler]\n",
    "y_train_shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Iterative classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract true labels\n",
    "ytest_true = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 54us/step\n",
      "Test set\n",
      "  Loss: 4.5661\n",
      "  Accuracy: 0.3139\n"
     ]
    }
   ],
   "source": [
    "accr = iter_model.evaluate(X_test, y_test, batch_size=512)\n",
    "print('Test set\\n  Loss: {:0.4f}\\n  Accuracy: {:0.4f}'.format(accr[0],accr[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 51us/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = iter_model.predict_classes(X_test, batch_size=512, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263/12263 [==============================] - 1s 51us/step\n",
      "Percentage of Positives: 0.1521\n"
     ]
    }
   ],
   "source": [
    "y_predscr = iter_model.predict(X_test, batch_size=512, verbose=1)\n",
    "\n",
    "predpos = np.count_nonzero(y_predscr[:,1] > 0.5)\n",
    "pred_allpos = predpos / len(y_predscr)\n",
    "print('Percentage of Positives: {:0.4f}'.format(pred_allpos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.31403408627578894\n",
      "Recall: 0.1727\n",
      "Precision: 0.9255091103965702\n",
      "F1 score: 0.2910837687510534\n",
      "\n",
      " clasification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.94      0.34      2263\n",
      "           1       0.93      0.17      0.29     10000\n",
      "\n",
      "    accuracy                           0.31     12263\n",
      "   macro avg       0.56      0.56      0.31     12263\n",
      "weighted avg       0.79      0.31      0.30     12263\n",
      "\n",
      "\n",
      " confusion matrix:\n",
      " [[2124  139]\n",
      " [8273 1727]]\n",
      "2124 139 8273 1727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06142288996906761"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lr = 0.005\n",
    "from sklearn.metrics import precision_score, \\\n",
    "    recall_score, confusion_matrix, classification_report, \\\n",
    "    accuracy_score, f1_score\n",
    "\n",
    "print('Accuracy:', accuracy_score(ytest_true, y_pred))\n",
    "print('Recall:', recall_score(ytest_true, y_pred))\n",
    "print('Precision:', precision_score(ytest_true, y_pred))\n",
    "print('F1 score:', f1_score(ytest_true, y_pred))\n",
    "print('\\n clasification report:\\n', classification_report(ytest_true, y_pred))\n",
    "print('\\n confusion matrix:\\n',confusion_matrix(ytest_true, y_pred))\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(ytest_true, y_pred).ravel()\n",
    "print(tn, fp, fn, tp)\n",
    "fpr = fp / (fp+tn)\n",
    "fpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0.031816173221387536,\n",
       " 0.052585064074237735,\n",
       " 0.05788775961113566,\n",
       " 0.058329650905877155,\n",
       " 0.06142288996906761,\n",
       " 0.062306672558550595,\n",
       " 0.06319045514803358,\n",
       " 0.06539991162174105,\n",
       " 0.06937693327441449,\n",
       " 1.0]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fp_classifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "    ytau_pred = (y_predscr[:,0] < tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "    fp_classifier.append(fpr_tmp)\n",
    "    \n",
    "fp_classifier.insert(0,0)\n",
    "fp_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0.7654, 0.7875, 0.8018, 0.8146, 0.8274, 0.8414, 0.8582, 0.8881, 0.976, 1.0]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# False negative rate: FNR=1âˆ’TPR or FNR=FN/(TP+FN) \n",
    "fn_classifier = []    \n",
    "percentages = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "for percent in percentages:\n",
    "    tau_threshold = percent\n",
    "#     ytau_pred = (y_predscr[:,0] < (1-tau_threshold)).astype(int)\n",
    "    ytau_pred = (y_predscr[:,1] > tau_threshold).astype(int)\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp = confusion_matrix(ytest_true, ytau_pred).ravel()\n",
    "    tn_tmp, fp_tmp, fn_tmp, tp_tmp \n",
    "    fpr_tmp = fp_tmp / (fp_tmp+tn_tmp)\n",
    "\n",
    "    fnr_tmp = 1 - fpr_tmp\n",
    "    fnr_tmp = fn_tmp / (tp_tmp+fn_tmp)\n",
    "    fn_classifier.append(fnr_tmp)\n",
    "    \n",
    "fn_classifier.insert(0,0)\n",
    "fn_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12263\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "scores = y_predscr[:,1] # classifier atk scores\n",
    "# def showscore(scores):\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "print(len(scores))\n",
    "plt.hist(scores, bins = 10, alpha=0.5, label='Pred Scores') \n",
    "#     plt.hist(scoretype[scores][scoretype['Attack'] == 1], bins = 10, alpha=0.5, label='attacker')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "    \n",
    "# showscore(y_predscr) # P scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.003\n",
    "\n",
    "Accuracy: 0.8068669527896996\n",
    "Recall: 0.6226241569589209\n",
    "Precision: 0.9859223300970874\n",
    "F1 score: 0.7632468996617813\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.99      0.84      3262\n",
    "           1       0.99      0.62      0.76      3262\n",
    "\n",
    "    accuracy                           0.81      6524\n",
    "   macro avg       0.86      0.81      0.80      6524\n",
    "weighted avg       0.86      0.81      0.80      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[3233   29]\n",
    " [1231 2031]]\n",
    "3233 29 1231 2031\n",
    "\n",
    "0.008890251379521767"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.0005\n",
    "Accuracy: 0.76931330472103\n",
    "Recall: 0.6489883507050889\n",
    "Precision: 0.8546628986677433\n",
    "F1 score: 0.7377591914967764\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.89      0.79      3262\n",
    "           1       0.85      0.65      0.74      3262\n",
    "\n",
    "    accuracy                           0.77      6524\n",
    "   macro avg       0.79      0.77      0.77      6524\n",
    "weighted avg       0.79      0.77      0.77      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[2902  360]\n",
    " [1145 2117]]\n",
    "2902 360 1145 2117\n",
    "\n",
    "0.11036174126302882"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# lr = 0.001\n",
    "Accuracy: 0.7967504598405886\n",
    "Recall: 0.6152667075413857\n",
    "Precision: 0.965832531280077\n",
    "F1 score: 0.751685393258427\n",
    "\n",
    " clasification report:\n",
    "               precision    recall  f1-score   support\n",
    "\n",
    "           0       0.72      0.98      0.83      3262\n",
    "           1       0.97      0.62      0.75      3262\n",
    "\n",
    "    accuracy                           0.80      6524\n",
    "   macro avg       0.84      0.80      0.79      6524\n",
    "weighted avg       0.84      0.80      0.79      6524\n",
    "\n",
    "\n",
    " confusion matrix:\n",
    " [[3191   71]\n",
    " [1255 2007]]\n",
    "3191 71 1255 2007\n",
    "\n",
    "0.021765787860208462"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def perf_measure(y_actual, y_hat):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    TN = 0\n",
    "    FN = 0\n",
    "\n",
    "    for i in range(len(y_hat)): \n",
    "        if y_actual[i]==y_hat[i]==1:\n",
    "            TP += 1\n",
    "        if y_hat[i]==1 and y_actual[i]!=y_hat[i]:\n",
    "            FP += 1\n",
    "        if y_actual[i]==y_hat[i]==0:\n",
    "            TN += 1\n",
    "        if y_hat[i]==0 and y_actual[i]!=y_hat[i]:\n",
    "            FN += 1\n",
    "\n",
    "    return(TP, FP, TN, FN)\n",
    "\n",
    "TP, FP, TN, FN = perf_measure(ytest_true, y_pred)\n",
    "FPR = FP / (FP+TN)\n",
    "FPR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ddos",
   "language": "python",
   "name": "ddos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
